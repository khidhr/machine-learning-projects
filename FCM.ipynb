{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy C-means\n",
    "\n",
    "Fuzzy C-means (FCM) is an unsupervised clustering algorithm widely used in machine learning and data analysis. It enhances the traditional C-means algorithm by incorporating fuzzy logic principles. Fuzzy logic allows for the maintenance of probabilistic assumptions, providing probabilities instead of definitive predictions.\n",
    "\n",
    "This integration of fuzzy logic in the prediction process is particularly valuable in situations where the accuracy of predictions carries significant implications. For instance, when diagnosing a patient as sick or healthy, instead of offering a binary prediction (true or false, 0 or 1), FCM with fuzzy logic generates probabilities. For example, the prediction might be something like [0.37, 0.63], indicating a 0.37 probability of the patient being sick and a 0.63 probability of the opposite case , the final decision will then be made by the doctor, who can consider these probabilities alongside other factors.\n",
    "\n",
    "By avoiding strict, misleading assumptions, FCM with fuzzy logic helps mitigate potential damaging consequences. It provides a more nuanced understanding of the data, allowing decision-makers to weigh probabilities and make safer informed choices based on the context and expertise.\n",
    "\n",
    "### Algorithm Steps:\n",
    "\n",
    "1. **Initialization:** Specify the number of desired clusters (c), the input dataset(X), the fuzziness parameter (m) and the maximum number of iterations(tmax) .\n",
    "\n",
    "2. **Membership Assignment:** Randomly select initial cluster centers coordinates between the maximums and the minumums of the provided data to accelerate the convergence and assign membership values to each data point. These values represent the degree of belongingness of each data point to each cluster. The formula to calculate the membership degree is :\n",
    "$$\\mu_{ij} = \\left(\\sum_{k=1}^{C} \\left(\\frac{d(x_i, v_j)}{d(x_i, v_k)}\\right)^{\\frac{2}{m-1}}\\right)^{-1}$$\n",
    "\n",
    "\n",
    "3. **Centroid Update:** Update the cluster centers (centroids) based on the weighted average of the data points, considering the membership values.The formula to calculate the centroids matrix is : \n",
    "$$ \n",
    "v_j = \\frac{\\sum_{i=1}^{N} \\left(\\mu_{ij}\\right)^m \\cdot x_i}{\\sum_{i=1}^{N} \\left(\\mu_{ij}\\right)^m}\n",
    "\n",
    "$$\n",
    "\n",
    "4. **Membership Update:** Recalculate the membership values for each data point based on their proximity to the updated centroids.\n",
    "\n",
    "5. **Convergence:** Iteratively perform centroid and membership updates until a stopping criterion is met.\n",
    "\n",
    "6. **Result:** Assign final membership values to each data point, indicating the degree of belongingness to each cluster.\n",
    "\n",
    "Fuzzy C-means allows for soft clustering, taking in consideration situations where data points may belong to multiple clusters with varying degrees of membership.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "D = iris.target\n",
    "n = X.shape[0]\n",
    "p = X.shape[1]\n",
    "c = 3\n",
    "\n",
    "class FCM:\n",
    "    def __init__(self, c, m, tmax, X):\n",
    "        '''\n",
    "        c:number of clusters\n",
    "        m: fuzziness parameter\n",
    "        tmax : maximum number of iterations\n",
    "        X : the data\n",
    "\n",
    "        '''\n",
    "        self.c = c\n",
    "        self.m = m\n",
    "        self.p = X.shape[1]\n",
    "        self.tmax = tmax\n",
    "        self.X = X\n",
    "        # calculating the mins and maxs of the data columns to initialise the centroids between those values.\n",
    "        min_ = [np.min(X[:,j]) for j in range(X.shape[1])]\n",
    "        max_ = [np.max(X[:,j]) for j in range(X.shape[1])]\n",
    "        # initializing each cluster centroid between the maxs and mins to accelerate the convergence\n",
    "        self.V = [[np.random.uniform(min_[j], max_[j]) for j in range(X.shape[1])] for i in range(c)]\n",
    "        '''\n",
    "        to measure the performance of our clustering technique , we will use a labeled data , but the problem is that we need to assign each cluster with the right class , the array Etqt will serve for that.\n",
    "        '''\n",
    "        self.Etqt = []\n",
    "        \n",
    "    def calculer_U(self):\n",
    "        '''\n",
    "        calculate the membership matrix U , its shape is (c,X.shape[0]) , so each column represent the degree of belongingness (probability) of each data point to each cluster\n",
    "        '''\n",
    "        a = -2 / (self.m - 1)\n",
    "        term_1 = [[np.power(np.linalg.norm(self.X[k]-self.V[i]), a) for k in range(self.X.shape[0])] for i in range(self.c)]\n",
    "        term_2 = np.sum(term_1, axis=0)\n",
    "        self.U =np.divide(term_1, term_2)\n",
    "        return self.U\n",
    "    \n",
    "    def calculer_V(self):\n",
    "        '''\n",
    "        V is the matrix of the different centroids ,its shape is (c,X.shape[1]) , the next method will update the centoids based on the new membership degrees\n",
    "        '''\n",
    "        term1 = np.dot(self.U**self.m,X)\n",
    "        term2 = np.sum(self.U**self.m, axis=1)\n",
    "        self.V = np.divide(term1.T,term2).T\n",
    "        return self.V\n",
    "        # \n",
    "    def etiqueterV(self):\n",
    "        ''' \n",
    "        to avaluate the performance of the model , we will test it on a labeled data and compare the predictions with the ground truth , but in order to do that we should first assign each cluster with the appropiate class , one way to do that is to calculate the distance between classes centers and clusters centroids and assign each cluster with nearest class.\n",
    "        the resulting array Etqt will have the clusters as values and the according classes as indexes , for example if Etqt = [2,0,1] , that means that the 3rd cluster reprsent the first class , the the first cluster represent the second class and the second cluster represent the third class.\n",
    "        '''\n",
    "        # compute the mean of each cluster\n",
    "        class_means = [np.mean(self.X[D == i], axis=0) for i in range(self.c)]\n",
    "        # find the closest centroid to each cluster mean\n",
    "        self.Etqt = []\n",
    "        for i in range(self.c):\n",
    "            centroid_distances = [np.linalg.norm(class_means[i] - self.V[j]) for j in range(self.c)]\n",
    "            closest_centroid = np.argmin(centroid_distances)\n",
    "            self.Etqt.append(closest_centroid)\n",
    "        \n",
    "    def classe(self, vect):\n",
    "        '''\n",
    "        the classe method will calculate the membership degrees of \"vect\" , predict wich cluster vect more likely belong to using the argmax() , and find the according class using the precalculated array Etqt , that way we can use performance metrics (in our case F1-score)to compare between the ground truth and the predicted classes.\n",
    "        '''\n",
    "        term1 = [ pow(sum(np.square(vect-self.V[i])), -1./(self.m-1)) for i in range(self.c)]\n",
    "        term2 = sum(term1)\n",
    "        u = [term1[i]/term2 for i in range(self.c)]\n",
    "        return self.Etqt.index(np.argmax(u)) # cluster of vect\n",
    "    \n",
    "    def fit(self):\n",
    "        '''\n",
    "        The usual fit method that runs the training , it will also include making the predictions since it is not necessary to make a methode for that\n",
    "        '''\n",
    "        for i in range(self.tmax):\n",
    "            self.calculer_U()\n",
    "            self.calculer_V()\n",
    "        self.etiqueterV()\n",
    "            #f1 = f1_score(D, y_pred, average='weighted')\n",
    "            #entropy = self.calculate_entropy(self.X)\n",
    "            # self.f1_scores.append(f1)\n",
    "            # self.entropies.append(entropy)\n",
    "        self.y_pred = [self.classe(x) for x in self.X]\n",
    "        print(np.asarray(self.y_pred))\n",
    "        print(self.Etqt)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Matrix of centroids before training ---\n",
      "[[5.94185194 3.88442231 2.17807531 1.33416265]\n",
      " [6.43269245 2.11148099 4.58451463 0.5092579 ]\n",
      " [4.53418573 4.27732529 6.697229   2.04015364]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2\n",
      " 2 2 1 2 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 2 2 2 2 1 2 2 2 1 2 2 2 1 2\n",
      " 2 1]\n",
      "[0, 1, 2]\n",
      "--- Matrix of centroids after training --- \n",
      "[[5.00396596 3.41408886 1.48281553 0.25354632]\n",
      " [5.88893236 2.76106936 4.36395164 1.39731504]\n",
      " [6.77501122 3.05238227 5.64678178 2.05354666]]\n"
     ]
    }
   ],
   "source": [
    "fcm = FCM(c=3, m=2, tmax=100, X=X)\n",
    "print('--- Matrix of centroids before training ---')\n",
    "print(np.asarray(fcm.V))\n",
    "fcm.fit()\n",
    "print('--- Matrix of centroids after training --- ')\n",
    "print(np.asarray(fcm.V))\n",
    "preds = fcm.y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 150)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcm.U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8944107744107744"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(preds,D,average='weighted')\n",
    "f1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that our model is doing well on the iris dataset obtaining 0.89 F1-score on classifying its instances ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
